# -*- coding: utf-8 -*-
"""PolyFarmsYT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kC854tG9W7zxbxAKf3KVFNUYkwabbULY
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import nltk

from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

import warnings
warnings.filterwarnings("ignore")

nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('stopwords')
nltk.download('vader_lexicon')

YT_data = pd.read_csv('PolyFarmsYT.csv')

YT_data.head()

#Data Cleaning
YT_data['Comments'] = YT_data['Comments'].str.replace("[^a-zA-Z#]", " ")
YT_data['Comments'] = YT_data['Comments'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))
YT_data['Comments'] = YT_data['Comments'].apply(lambda x:x.lower())

YT_data.head()

tokenized_comments = YT_data['Comments'].apply(lambda x: x.split())
tokenized_comments.head()

wnl = WordNetLemmatizer()

sw = set(stopwords.words('english'))

tokenized_comments.apply(lambda x: [wnl.lemmatize(i) for i in x if i not in sw])
tokenized_comments.head()

for i in range(len(tokenized_comments)):
    tokenized_comments[i] = ' '.join(tokenized_comments[i])

tokenized_comments

YT_data['Comments'] = tokenized_comments

YT_data.head()

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()

YT_data['Sentiment Scores'] = YT_data['Comments'].apply(lambda x:sia.polarity_scores(x)['compound'])

YT_data.head()

YT_data['Sentiment'] = YT_data['Sentiment Scores'].apply(lambda s : 'Positive' if s > 0 else ('Neutral' if s == 0 else 'Negative'))

YT_data.head()

YT_data['Sentiment'].unique()

YT_data['Sentiment'].value_counts()

a = YT_data[(YT_data.Sentiment == 'Positive')].count()[0]
b = YT_data[(YT_data.Sentiment == 'Negative')].count()[0]
c = YT_data[(YT_data.Sentiment == 'Neutral')].count()[0]

Total = YT_data['Sentiment'].value_counts().sum()

Percentage = (a/Total)*100

print("Sentiment Score for Youtube Comments:")
print(Percentage)

y = round(YT_data['Sentiment'].value_counts(normalize=True)*100,2)
mylabels = ["Positive", "Neutral", "Negative"]

plt.pie(y, autopct='%1.1f%%')
plt.legend(mylabels)

plt.bar(mylabels, YT_data['Sentiment'].value_counts())
plt.title("Bar plot of Comment Sentiments")

plt.barh(mylabels, YT_data['Sentiment'].value_counts())
plt.title("Bar plot of Comment Sentiments")

all_words = ' '.join([text for text in YT_data['Comments']])
from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

all_words_posi = ' '.join([text for text in YT_data['Comments'][YT_data.Sentiment == 'Positive']])

wordcloud_posi = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words_posi)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud_posi, interpolation="bilinear")
plt.axis('off')
plt.title('Positive Word Cloud')
plt.show()

all_words_nega = ' '.join([text for text in YT_data['Comments'][YT_data.Sentiment == 'Negative']])

wordcloud_nega = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words_nega)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud_nega, interpolation="bilinear")
plt.axis('off')
plt.title('Neagtive Word Cloud')
plt.show()

